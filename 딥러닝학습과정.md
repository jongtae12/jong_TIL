# 딥러닝 학습과정
## 모델 생성
pytorch라이브러리인 torch를 import, 신경망 관련 기능인 torch.nn을 import  
```
import torch
import torch.nn as nn
```
**모델 클래스 정의**  
```
class SampleModel(nn.Module):   # nn.Module을 상속받아서 신경망 모델 정의
    def __init__(self): # 생성자 함수 만들기
        super().__Init__()
        self.linear =nn.Linear(1,1)
        # 현재 상속받은 클래스의 init함수 호출(부모 클래스,nn.module의 init 호출)
        #가중치,편향 자동 생성

        def forward(self,x): # 순전파 함수
            return self.linear(x)
```
```
model = SampleModel() # 모델 설정
criterion = nn.MSELoss() # mse 계산산
optimizer = optim.SGD(model.parameters(), lr=0.01) # 가중치를 업그레이드 할 때 0.01만큼 조정
```
```
# 학습데이터 생성성
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])

y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])
```
```
# 예측값
y_pred = model(x_train)
```
```
loss = criterion(y_pred, y_train) # 손실함수 계산
print(f'손실>>>>>>{loss}:{loss.item():.4f}')
```
```
optimizer.zero_grad() # 이전 기울기 초기화화

loss.backward() $ 역전파파
```
```
for name, parm in model.named_parameters(): # 가중치,편향 값 구하기기
    print(f'{name}이 아이의 기울기>>>> {parm.grad}')
```
```
optimizer.step() # 가중치 업데이트
```
이와 같이 순전파,손실 함수 계산,역전파,가중치 업데이트 순으로 이루어진다.