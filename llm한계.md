# llm 한계


- 프롬프트 주입 (Prompt Injection)  

악의적인 프롬프트(질문)를 입력하여 LLM이 본래 정책이나 가이드라인에  
구애받지 않고 공격자 의도대로 작동하도록 하는 취약점이다. LLM 접근 권한  
제어 강화와 외부 콘텐츠 분리 등을 통해 완화할 수 있다. 


- 불완전한 출력 처리 (Insecure Output Handling)  


LLM에서 생성된 출력이 충분한 검증 과정 없이 다른 시스템으로 전달될 경우,  
원격 코드 실행 등의 위협이 발생할 수 있다. 제로 트러스트 접근 방식 사용과 입력  
유효성 검사 등을 통해 예방할 수 있다.  


- 학습 데이터 중독 (Training Data Poisoning)  


사전 학습 데이터를 조작하여 모델의 보안성과 효율성을 손상시키는  취약점이다.  
사용자가 오염된 정보에 노출되거나 시스템 성능 저하를 초래할  수 있다. 안정성이  
검증된 학습 데이터를 사용하여 예방할 수 있다.  


- 모델 서비스 거부 (Model Denial of Service)  


공격자가 대량의 리소스를 소모시켜 다른 사용자의 서비스 품질을 저하시키고  
높은 리소스 비용을 발생시킨다. 사용자 입력 제한 규칙 준수와 리소스 사용량   
제한 등을 통해 예방할 수 있다.  


- 공급망 취약점 (Supply Chain Vulnerabilities)  


체계적인 방식이나 도구 없이 LLM 공급망을 관리하기 어렵기 때문에,  
소프트웨어 공급망 취약점과 유사한 위협이 발생할 수 있다.  
신뢰할 수 있는 공급 업체 사용과 패치 정책 구현 등을 고려해야 한다.  


- 민감 정보 노출 (Sensitive Information Disclosure)  

LLM의 답변을 통해 민감한 정보가 노출될 수 있으며, 이로 인해 개인 정보 침해나 지적  
재산에 대한 무단 액세스가 발생할 수 있다. 적절한 데이터 정제 기술을 사용하여 민감  
데이터가 학습 데이터에 포함되지 않도록 해야 한다.  


- 불완전 플러그인 설계 (Insecure Plugin Design)  

LLM 플러그인은 사용자가 다른 애플리케이션 사용 중 자동으로 호출되는 확장 기능이다.  
모델이 다른 플랫폼에서 제공될 경우 애플리케이션 실행을 제어할 수 없으므로 원격 코드  
실행 등의 위협이 발생할 수 있다. 민감한 작업 실행 시 수동 승인을 요구하고 인증 ID를  
적용하는 등의 방법으로 예방할 수 있다.  


- 과도한 에이전시 (Excessive Agency)  


기능 호출 권한을 가진 에이전트가 LLM의 출력에 대응하여 해로운 작업을 수행할 수 있다.  
세분화된 기능을 갖춘 플러그인을 사용하고 최소한의 권한으로 제한하는 등의 방법으로 예방할 수 있다.  


- 과도한 의존 (Overreliance)  


LLM이 사실과 다른 정보나 부적절한 콘텐츠를 생성하는 환각 현상이 발생할 수 있다.  
이러한 결과를 검토하지 않고 신뢰하면 잘못된 정보가 전달될 수 있다. 파인튜닝, 임베딩, RAG 기법 등을  
활용하여 출력 품질을 개선하고, 전문가 검증을 받으며, 사용자가 LLM의 한계를 명확히  
인식하도록 하는 방법으로 예방할 수 있다.

- 모델 도난 (Model Theft)  


공격자가 해킹을 통해 LLM 모델에 무단으로 접근하거나 모델이 유출될 수 있다.  
강력한 보안 조치를 통해 예방할 수 있다.